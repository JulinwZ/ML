{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13d68de6",
      "metadata": {
        "id": "13d68de6"
      },
      "source": [
        "# Лабораторная работа 4. Полносвязные нейронные сети (многослойный персептрон). Решение задач регрессии и классификации"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f92923",
      "metadata": {
        "id": "e9f92923"
      },
      "source": [
        "## Искусственные нейроны"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1a07e2",
      "metadata": {
        "id": "8d1a07e2"
      },
      "source": [
        "Искусственными нейронными сетями (чаще - просто нейронными сетями) называются модели машинного обучения, в основе функционирования которых лежат <b>принципы работы биологических нейронов</b> в человеческом мозге."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8517f9",
      "metadata": {
        "id": "fa8517f9"
      },
      "source": [
        "Идея, лежащая в основе нейронных сетей, очень простая: каждый биологический нейрон имеет несколько входов (дендритов), на основе информации с которых формируется выходной сигнал, который с помощью выхода (аксона) передается далее к органам человеческого (и не только) организма. В 1943 году У. Маккалок и У. Питтс предложили идею искусственного нейрона."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea8fb4c",
      "metadata": {
        "id": "7ea8fb4c"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/ru/thumb/b/ba/Single_layer_perceptron.png/270px-Single_layer_perceptron.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f99850",
      "metadata": {
        "id": "38f99850"
      },
      "source": [
        "Искусственный нейрон также имеет дендриты и аксон. Математически, здесь на вход нейрона подается некоторый вектор из чисел $x$. При этом каждый дендрит имеет свой вес $w$. Значение нейрона $h$ вычисляется как $h=wx^T$, если в векторной форме. А если в скалярной, то речь идет о простом перемножении компонент входного вектора на соответствующие веса связей и последующее суммирование."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1e50bf",
      "metadata": {
        "id": "7c1e50bf"
      },
      "source": [
        "Заметим, что такой нейрон полностью эквивалентен линейному регрессору, а это значит, что он может находить в данных исключительно линейные зависимости. Чтобы такого не было придумали передавать аксону не $h$, а $f(h)$, где $f$ - нелинейная функция, называемая <b>функцией активации</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f41ff97",
      "metadata": {
        "id": "7f41ff97"
      },
      "source": [
        "Функции активации способны управлять множеством значений нейрона. Ниже приведены некоторые функции активации."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf033d03",
      "metadata": {
        "id": "bf033d03"
      },
      "source": [
        "![](https://programforyou.ru/images/useful/cnn/part0/activations.png?v=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67a4f37",
      "metadata": {
        "id": "d67a4f37"
      },
      "source": [
        "## Полносвязные нейронные сети. Получение предсказаний"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a2cce9",
      "metadata": {
        "id": "52a2cce9"
      },
      "source": [
        "Со временем идея искусственных нейронов была обобщена. Появились модели, в которых уже присутствовало несколько взаимосвязанных между собой нейронов. Исторически первым прикладным обобщением сетей из искусственных нейронов является <b>многослойный персептрон</b>. Его концепция была предложена Ф. Розенблатом в 1958 году. Однако персептрон Розенблата имел всего три слоя. Мы же будем рассматривать обобщенную модель."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d046b4bb",
      "metadata": {
        "id": "d046b4bb"
      },
      "source": [
        "![](https://neerc.ifmo.ru/wiki/images/thumb/6/63/Multi-layer-neural-net-scheme.png/500px-Multi-layer-neural-net-scheme.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea7d5d8",
      "metadata": {
        "id": "6ea7d5d8"
      },
      "source": [
        "Представленая выше нейронная сеть (многослойный персептрон) называется <b>полносвязной</b>. Это означает, что каждый нейрон текущего слоя связан с каждым нейроном предыдущего слоя. Если скрытых нейронов больше чем один, то такая сеть называется <b>глубокой</b>. Обучение глубоких нейронных сетей называется глубоким обучением."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4485ea9e",
      "metadata": {
        "id": "4485ea9e"
      },
      "source": [
        "В сети выделяют входной слой (нейроны, на которых просто размещается вектор входных значений), скрытые слои (у каждого слоя своя функция активации, которую используют все нейроны) и выходной слой (функция активации выходного слоя отображает значения суммы в требуемое множество значений). Вы, наверное, догадались, что в случае регрессии функция активации выходного слоя, как правило, не ограничивает значения нейронов (то есть может вообще не применяться), может отображать значения нейрона в положительное число (например, relu). В случае классификации в большинстве случаев используются функции активации sigmoid и softmax. Как именно они применяются вы увидите на практике ниже."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3c0722",
      "metadata": {
        "id": "ad3c0722"
      },
      "source": [
        "Получение предсказаний с помощью нейронной сети - это <b>процесс последовательного выполнения матричного умножения с последующим поэлементным применением функции активации к получившемуся вектору<b>. Не верите? давайте это увидим."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dc34f6",
      "metadata": {
        "id": "d5dc34f6"
      },
      "source": [
        "Каждому слою сети (кроме входного) соответствует матрица обучаемых параметров. Пусть мы рассматриваем первых скрытый слой, обозначим количество его нейронов за $m$. Обозначим количество нейронов предыдущего слоя (входного) за $n$. Тогда матрица весов слоя $W$ будет иметь размерность $m{\\times}n$. Элемент $w_{ij}$ - вес связи $i$ нейрона текущего слоя с $j$ нейроном предыдущего."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442c1622",
      "metadata": {
        "id": "442c1622"
      },
      "source": [
        "Также каждому слою соответствует собственный вектор $b$ - это значение сдвига. Количество элементов вектора b соответствует количеству нейронов текущего слоя. Значения нейронов текущего слоя $h$ вычисляется как $h=Wx+b$. Выходное значение нейронов вычисляется как $f(h)$, где $f$ - функция активации текущего слоя."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63a2d7a",
      "metadata": {
        "id": "b63a2d7a"
      },
      "source": [
        "Как вы видите, получить предсказания очень просто. Изначально значения W и b каждого слоя инициализируются случайным образом. Вы понимаете, что для получения адекватных предсказаний нам необходимо выполнить обучение, то есть <b>найти значения W и b для каждого слоя сети, которые позволят минимизировать функцию ошибки</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7395af",
      "metadata": {
        "id": "4c7395af"
      },
      "source": [
        "## Обучение полносвязных нейронных сетей. Алгоритм обратного распространения ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431697ea",
      "metadata": {
        "id": "431697ea"
      },
      "source": [
        "Обучение нейронной сети производится с использованием подходов, в основе которых лежит градиентный спуск. В самом простом случае - это обычный, уже знакомый нам, метод наискорейшего спуска. Но вот задача - все эти методы требуют расчета градиента функции ошибки. А как нам посчитать градиент функции ошибки при использовании нейронной сети? Оказывается, что в этом случае мы не можем просто взять и посчитать сразу весь градиент. Вместо этого, мы можем вычислить его по отдельным частям. Алгоритм вычисления градиента, используемый при обучении нейронных сетей, получил название <b>метод обратного распространения ошибки (backpropagation)</b>. Понимание его работы - это основа вашего понимания работы нейронных сетей."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a01ba",
      "metadata": {
        "id": "376a01ba"
      },
      "source": [
        "Суть метода обратного распространения ошибки заключается в том, что мы после получения конечных предсказаний начинаем идти назад (от последнего слоя к первому) и последовательно вычислять части градиента. Как вы, наверное, догадались, обучаемыми параметрами у нас являются $W$ и $b$ для каждого слоя. Мы двигаемся начиная с последнего слоя и последовательно вычисляем эти градиенты."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a241e8a",
      "metadata": {
        "id": "7a241e8a"
      },
      "source": [
        "Для того, чтобы нам удобно было все это понять, давайте каждый слой разобьем еще на два слоя. Для простоты сделаем так, что вся сеть имеет только входной слой и выходной (выходной разбит на два отдельных слоя)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4a7d99",
      "metadata": {
        "id": "eb4a7d99"
      },
      "source": [
        "![](https://i.vgy.me/S1IeLk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e36a3e",
      "metadata": {
        "id": "a8e36a3e"
      },
      "source": [
        "Мы видим, что любую нейронную сеть таким образом можно разбить на большее количество слоев, если каждый слой с функцией активации разбить на два слоя. Представить, что на первом выполняется суммирование произведений (умножение матрицы на вектор и добавление сдвига), а на втором - поэлементное применение функции активации. Продолжая подобные рассуждения, мы придем к тому, что любая полносвязная сеть может быть представлена как набор последовательных компонентов, каждый из которых можно рассматривать как функцию."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11839a14",
      "metadata": {
        "id": "11839a14"
      },
      "source": [
        "![](https://i.vgy.me/7vWpIw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c3a7c6",
      "metadata": {
        "id": "05c3a7c6"
      },
      "source": [
        "Каждый такой компонент берет входные данные и преобразует их в выходные (в случае полносвязной нейронной сети компонент либо выполняет линейное преобразование, либо применяет функцию активации). Входные данные текущего блока являются выходными данными предыдущего. Но посмотрите, а что представляют собой тогда выходные данные последнего компонента с математической точки зрения? Это ни что иное, как результат применения <b>сложной функции</b> к входным данным. В данном случае, $y_3=f_3(f_2(f_1(x_1)))$. А теперь давайте вспомним, что каждый эти компоненты содержат обучаемые параметры $W$ и $b$. В данном случае у нас есть $W_1$ и $b_1$, а также $W_3$ и $b_3$ (второй компонент не содержит обучающих параметров, поскольку отвечает за просто поэлементное применение функции активации)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be6d5ee",
      "metadata": {
        "id": "6be6d5ee"
      },
      "source": [
        "Но мы с вами ранее сказали о том, что градиент вычисляется частично и по каждому множеству обучаемых параметров, так? Да, все именно так. Мат. анализ предоставляет нам замечательный инструмент для вычисления производных сложной функции - <b>цепное правило (chain rule)</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7594ccb",
      "metadata": {
        "id": "f7594ccb"
      },
      "source": [
        "Суть цепного правила вы помните со школы: если $y=y(g(x))$, то ${\\frac{dy}{dx}}={\\frac{dy}{dg}}{\\frac{dg}{dx}}$. То же самое работает и в нашем случае. Каждый компонент использует значения частных производных функции потерь по своему выходу для непосредственного вычисления частных производных функции потерь по $W$ и $b$, а также передает предыдущему компоненту вычисленные значения производной функии потерь по своему входу. Далее компонент с использованием оптимизатора делает шаг градиентного спуска (обновляются значения весов $W$ и $b$). <b>Обращаю внимание: обновление весов выполняется ПОСЛЕ вычисления частных производных по весам</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82dea9a2",
      "metadata": {
        "id": "82dea9a2"
      },
      "source": [
        "![sejsej](https://i.vgy.me/m5KKFF.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fd14a4",
      "metadata": {
        "id": "98fd14a4"
      },
      "source": [
        "Итак, пусть у нас задана функция потерь. Для регрессии и бинарной классификации можно использовать модифицированную MSE: $E={\\frac{1}{2}}(y-\\hat{y})^2$. Мы хотим ее минимизировать. Ранее мы буквой $L$ обозначали функцию потерь, а при работе с нейронными сетями устоялось обозначение $E$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca869d1e",
      "metadata": {
        "id": "ca869d1e"
      },
      "source": [
        "Осталось разобраться, по каким формулам вычисляются части градиента в каждом компоненте. Мы знаем, что в каждом компоненте вычисляется $\\frac{\\partial{E}}{\\partial{x}}$. В некоторых компонентах вычисляются $\\frac{\\partial{E}}{\\partial{W}}$ и $\\frac{\\partial{E}}{\\partial{b}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733f597a",
      "metadata": {
        "id": "733f597a"
      },
      "source": [
        "Запишем формулы:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad6e1a1",
      "metadata": {
        "id": "dad6e1a1"
      },
      "source": [
        "$\\frac{\\partial{E}}{\\partial{W}}$ = $\\frac{\\partial{E}}{\\partial{y}}x^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe2984a",
      "metadata": {
        "id": "abe2984a"
      },
      "source": [
        "$\\frac{\\partial{E}}{\\partial{b}}$ = $\\frac{\\partial{E}}{\\partial{y}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc66a9d5",
      "metadata": {
        "id": "dc66a9d5"
      },
      "source": [
        "Частные производные функции потерь по входу вычисляются по разному, в зависимости от назначения компонента. Если это компонент, реализующий линейное преобразование ($Wx+b$), то $\\frac{\\partial{E}}{\\partial{x}} = W^T\\frac{\\partial{E}}{\\partial{y}} $. Если это компонент, применяющий функцию активации $f$ (sigmoid, tanh, relu), то $\\frac{\\partial{E}}{\\partial{x}} = \\frac{\\partial{E}}{\\partial{y}}\\odot f'(x) $. $\\odot$ - это поэлементное произведение векторов (произведение Адамара)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "122831c5",
      "metadata": {
        "id": "122831c5"
      },
      "source": [
        "Можно увидеть, что все функции активации слоев обязаны быть дифференцируемыми."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0382aa47",
      "metadata": {
        "id": "0382aa47"
      },
      "source": [
        "При решении задач классификации (в общем случае, когда количество классов больше двух), как правило, применяется функция потерь перекрестная энтропия (при бинарной классификации применяется ее частный случай - бинарная перекрестная кросэнтропия). Выглядит она следующим образом:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0701bb5a",
      "metadata": {
        "id": "0701bb5a"
      },
      "source": [
        "Общий случай: $E = -\\sum_{k}^{s}{{y_k}ln{\\hat{y_k}}}$. Здесь s - количество классов. При использовании такой функции потерь, предполагается, что целевой признак размечен (например, для случая двух классов) как [1, 0]. Это оначает, что объект относится к 0 классу. Предположим, модель предсказала ответ [0,36, 0,64]. Она ошиблась. Можно посчитать значение перекрестной энтропии и обновить веса."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ea437c",
      "metadata": {
        "id": "d8ea437c"
      },
      "source": [
        "Как вы видите, использование перекрестной энтропии требует, чтобы сумма значений нейронов была единица и все числа были положительными. Для получения такого результата на произвольном слое с нейронами используется функция softmax. Ее можно назвать функцией активации, однако при использовании softmax $\\frac{\\partial{E}}{\\partial{x}}$ считается по другому. $\\frac{\\partial{E}}{\\partial{x}} = ((1-y^T)y)\\frac{\\partial{E}}{\\partial{y}} $. Подчеркну, что 1 здесь обозначена единичная матрица."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "647ebb36",
      "metadata": {
        "id": "647ebb36"
      },
      "source": [
        "Вот и все. Теперь вы можете реализовать собственную полносвязную нейронную сеть. Для облегчения задачи представьте ее в виде компонентов, как это описано выше. Чтобы начать вычисление градиента необходимо начать двигаться от последнего компонента к первому, при этом предварительно посчитать частную производную функции потерь по выходу последнего компонента."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb42973",
      "metadata": {
        "id": "adb42973"
      },
      "source": [
        "## Использование фреймворка TensorFlow и API Keras для построеония нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08b38f0",
      "metadata": {
        "id": "b08b38f0"
      },
      "source": [
        "Мы разобрались, как работают полносвязные нейронные сети. Давайте теперь решим задачи регрессии и классификации с помощью фреймворка TensorFlow. Начнем с загрузки предварительно обработанных данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5340d31a",
      "metadata": {
        "id": "5340d31a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_regression = pd.read_csv(\"../data/regression/apartment_data_preprocessed.csv\")\n",
        "data_classification = pd.read_csv(\"../data/classification/bank_churners_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d04b7d",
      "metadata": {
        "id": "a0d04b7d"
      },
      "outputs": [],
      "source": [
        "data_regression.drop(columns = [\"Unnamed: 0\"], inplace=True)\n",
        "data_classification.drop(columns = [\"Unnamed: 0\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6bc6d4",
      "metadata": {
        "id": "ef6bc6d4",
        "outputId": "61070045-2298-4c89-e866-2bf53ba5b41b"
      },
      "outputs": [],
      "source": [
        "data_regression.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df0807a",
      "metadata": {
        "id": "9df0807a",
        "outputId": "6e1a6123-a978-4278-fd47-6834d9096dfb"
      },
      "outputs": [],
      "source": [
        "data_classification.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc58fb63",
      "metadata": {
        "id": "dc58fb63"
      },
      "outputs": [],
      "source": [
        "y_regression = data_regression[\"SalePrice\"]\n",
        "X_regression = data_regression.drop(columns = ['SalePrice'])\n",
        "y_classification = data_classification['Attrition_Flag']\n",
        "X_classification = data_classification.drop(columns = ['Attrition_Flag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5cf5839",
      "metadata": {
        "id": "f5cf5839"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_regression_train, X_regression_test, y_regression_train, y_regression_test = train_test_split(X_regression,\n",
        "                                                                                                y_regression,\n",
        "                                                                                                test_size=0.2)\n",
        "X_classification_train, X_classification_test, y_classification_train, y_classification_test = train_test_split(X_classification,\n",
        "                                                                                                                y_classification,\n",
        "                                                                                                                stratify=y_classification,\n",
        "                                                                                                                test_size=0.2)\n",
        "                                                                                                                \n",
        "                                                                                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b84f4fc",
      "metadata": {
        "id": "9b84f4fc"
      },
      "source": [
        "Импортируем метрики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fda21ed2",
      "metadata": {
        "id": "fda21ed2"
      },
      "outputs": [],
      "source": [
        "# для оценки качества решения задачи регрессии\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# для оценки качества решения задачи классификации\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "078d1a1c",
      "metadata": {
        "id": "078d1a1c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49610a3",
      "metadata": {
        "id": "f49610a3"
      },
      "source": [
        "### Регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e563d4d",
      "metadata": {
        "id": "0e563d4d"
      },
      "source": [
        "Создаем полносвязную нейронную сеть для решения задачи регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c73d33",
      "metadata": {
        "id": "b3c73d33"
      },
      "outputs": [],
      "source": [
        "# создаем модель, как набор последовательных слоев\n",
        "model_regression = tf.keras.Sequential(\n",
        "    [\n",
        "        # Dense - полносвязный слой (каждый нейрон следующего слоя связан со всеми нейронами предыдущего)\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(40,)),\n",
        "        # на втором скрытом слое будет 32 нейрона\n",
        "        tf.keras.layers.Dense(32, activation=\"linear\"),\n",
        "        # Dropout позволяет внести фактор случайности - при обучении часть нейронов будет отключаться\n",
        "        # каждый нейрон, в данном случае, будет отключаться с вероятностью 0.1\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        # на выходе один нейрон, функция активации не применяется\n",
        "        tf.keras.layers.Dense(1, activation=\"linear\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af49e8d0",
      "metadata": {
        "id": "af49e8d0",
        "outputId": "eaee9795-f3d3-4e99-d4f3-f218f0440c11"
      },
      "outputs": [],
      "source": [
        "# посмотрим, какая сеть у нас получилась\n",
        "model_regression.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04c118e",
      "metadata": {
        "id": "f04c118e"
      },
      "source": [
        "Видим количество обучаемых параметров каждого слоя и общее количество обучаемых параметров. Перед использованием модель необходимо скомпилировать, при этом указывается оптимизатор, скорость обучения (можно представлять как величину шага в методе градиентного спуска), функция потерь и метрики, которые мы хотим (при желании) вычислять в будущем методом evaluate()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ebc13f",
      "metadata": {
        "id": "11ebc13f"
      },
      "outputs": [],
      "source": [
        "# компилируем\n",
        "model_regression.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=\"mse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d03d027",
      "metadata": {
        "id": "5d03d027",
        "outputId": "66fc9b4f-897f-4f1d-b43d-0699ba361f00"
      },
      "outputs": [],
      "source": [
        "# обучаем, 10 эпох означает 10 проходов по обучающей выборке\n",
        "model_regression.fit(X_regression_train, y_regression_train, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe0b9d4",
      "metadata": {
        "id": "2fe0b9d4",
        "outputId": "5e2c5afc-20ba-44c0-b3a4-c7417f1d97fd"
      },
      "outputs": [],
      "source": [
        "# оцениваем качество с помощью метрик\n",
        "print(mean_absolute_error(y_regression_test, model_regression.predict(X_regression_test)))\n",
        "print(mean_squared_error(y_regression_test, model_regression.predict(X_regression_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5399f5ba",
      "metadata": {
        "id": "5399f5ba"
      },
      "source": [
        "Мы получили наглядную демонстрацию важного факта - в некоторых задачах применение нейронных сетей менее целесообразно, чем использование более простых моделей. Но иногда они дают лучшие результаты. Важную роль еще играет подбор архитектуры и параметров."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4838f599",
      "metadata": {
        "id": "4838f599"
      },
      "source": [
        "### Бинарная классификация"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5c0f56",
      "metadata": {
        "id": "9b5c0f56"
      },
      "source": [
        "Нейронная сеть для решения задачи классификации будет очень похожа на ту сеть для регрессии, однако у нее по другому будет организован выходной слой. У нас есть 2 стратегии наполнения выходного слоя нейронами:\n",
        "\n",
        "- при решении задачи бинарной классификации мы можем расположить на выходном слое один нейрон с функцией активации sigmoid (значения от 0 и 1), после чего округлять полученные значения; значение нейрона покажет уверенность сети в предсказании; также мы можем расположить 2 нейрона на выходном слое и применить функцию softmax. Тогда сумма значений нейронов выходного слоя будет 1, а предсказание мы сможем получить определив нейрон с наибольшим значением;\n",
        "- в случае многоклассовой классификации, как правило, на выходном слое располагаются k нейронов (по количеству классов), функция активации - softmax; нейрон с наибольшим значением определяет предсказанный класс."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdc09fe",
      "metadata": {
        "id": "cfdc09fe"
      },
      "source": [
        "У нас задача бинарной классификации, попробуем обе стратегии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66cf4a4",
      "metadata": {
        "id": "a66cf4a4",
        "outputId": "b4c9aa05-6b9f-44e8-8086-03a87f6aa200"
      },
      "outputs": [],
      "source": [
        "model_classification_1 = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(23,)),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.05),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        # сначала используем 1 нейрон и sigmoid\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "# в качестве функции активации используется бинарная  кроссэнтропия\n",
        "model_classification_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "# verbose=None - не будет логов\n",
        "model_classification_1.fit(X_classification_train, y_classification_train, epochs=25, verbose=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e63257",
      "metadata": {
        "id": "46e63257"
      },
      "source": [
        "Посмотрим, как выглядят предсказания сети."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18172a49",
      "metadata": {
        "id": "18172a49",
        "outputId": "5788d18f-ecc8-44f1-950a-cf5d4445dfdd"
      },
      "outputs": [],
      "source": [
        "model_classification_1.predict(X_classification_test, verbose=None)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef5c89e",
      "metadata": {
        "id": "8ef5c89e"
      },
      "source": [
        "Это числа от 0 до 1, поскольку мы использовали sigmoid. Для того, чтобы получить финальное предсказания классов, необходимо округлить все полученные значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161a4c98",
      "metadata": {
        "id": "161a4c98",
        "outputId": "3fcfcaa3-650b-431c-d32b-d5626fa9ee0c"
      },
      "outputs": [],
      "source": [
        "y_pred = np.around(model_classification_1.predict(X_classification_test, verbose=None))\n",
        "print(classification_report(y_classification_test, y_pred))\n",
        "print(confusion_matrix(y_classification_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048b27da",
      "metadata": {
        "id": "048b27da"
      },
      "source": [
        "Обратите внимание, что дисбаланс классов может привести к неудовлетворительным результатам обучения модели. Необходимо сбалансировать обучающую выборку."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43bc4e63",
      "metadata": {
        "id": "43bc4e63"
      },
      "source": [
        "Но, даже без выполнения балансировки, можно взвесить функцию потерь. Можем указать веса (параметр class_weight), которые будут использоваться при оптимизации функции ошибки. В качестве весов классов можно задать величины, обратные количеству элементов класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df16179e",
      "metadata": {
        "id": "df16179e"
      },
      "outputs": [],
      "source": [
        "w0 = 1 / y_classification_train[y_classification_train==0].shape[0]\n",
        "w1 = 1 / y_classification_train[y_classification_train==1].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efce734",
      "metadata": {
        "id": "1efce734",
        "outputId": "afe7e5f2-f0c2-45a3-f63b-7c61208df0c6"
      },
      "outputs": [],
      "source": [
        "model_classification_1 = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(23,)),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.05),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        # используем 1 нейрон и sigmoid\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "model_classification_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=\"binary_crossentropy\")\n",
        "model_classification_1.fit(X_classification_train, y_classification_train, epochs=25, verbose=None,\n",
        "                           class_weight={0: w0, 1: w1})\n",
        "y_pred = np.around(model_classification_1.predict(X_classification_test, verbose=None))\n",
        "print(classification_report(y_classification_test, y_pred))\n",
        "print(confusion_matrix(y_classification_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370e3660",
      "metadata": {
        "id": "370e3660"
      },
      "source": [
        "Видим улучшения. Можем поиграть с архитектурой и параметрами и добиться еще более качественных результатов. Но напоследок давайте попробуем разместить 2 нейрона на выходном слое и использовать softmax в качестве функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd17e1c4",
      "metadata": {
        "id": "cd17e1c4",
        "outputId": "56b71377-8862-48d7-d8fb-4829c6ca224c"
      },
      "outputs": [],
      "source": [
        "model_classification_2 = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(23,)),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.05),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        # сначала используем 2 нейрона и softmax\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "# в качестве функции активации используется категориальная кроссэнтропия\n",
        "# используем разряженный (sparse) вариант, поскольку значения целевого признака не закодированы One-Hot кодированием\n",
        "model_classification_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=\"sparse_categorical_crossentropy\")\n",
        "model_classification_2.fit(X_classification_train, y_classification_train, epochs=25, verbose=None,\n",
        "                           class_weight={0: w0, 1: w1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b44a42",
      "metadata": {
        "id": "34b44a42",
        "outputId": "d34e5d71-0e72-4ab9-ca6c-1af9d6826d73"
      },
      "outputs": [],
      "source": [
        "model_classification_2.predict(X_classification_test, verbose=None)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2f88855",
      "metadata": {
        "id": "c2f88855"
      },
      "source": [
        "Каждое предсказание - это два числа (потому что два нейрона). Сумма значений равна 1. Каждое значение можно интерпретировать как вероятность отнесения объекта к соответствующему классу (0 или 1). Воспользуемся функцией argmax для того, чтобы получить итоговые предсказания."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2143a1ef",
      "metadata": {
        "id": "2143a1ef"
      },
      "outputs": [],
      "source": [
        "# получим индексы максимального значения для каждого элемента (вложенный массив) с помощью numpy\n",
        "y_pred = [np.argmax(pred) for pred in model_classification_2.predict(X_classification_test, verbose=None)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87091ba5",
      "metadata": {
        "id": "87091ba5",
        "outputId": "4ed37b0f-dc35-4c5a-ab0b-51bee6776817"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_classification_test, y_pred))\n",
        "print(confusion_matrix(y_classification_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2469c34e",
      "metadata": {
        "id": "2469c34e"
      },
      "source": [
        "Когда мы закончили обучение моделей, мы можем сохранить их на диск, чтобы в будущем либо продолжить обучение (если оно занимает много времени) или использовать для получения предсказаний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0ea2e8",
      "metadata": {
        "id": "5a0ea2e8",
        "outputId": "67fdc99e-ce2c-4a6d-b174-2e324a94270c"
      },
      "outputs": [],
      "source": [
        "model_regression.save('../models/RegressionModel')\n",
        "model_classification_1.save('../models/ClassificationModel1')\n",
        "model_classification_2.save('../models/ClassificationModel2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29fc0974",
      "metadata": {
        "id": "29fc0974"
      },
      "source": [
        "Модели сохранены в виде папки. Теперь, когда они нам потребуются, можем очень просто их загрузить. Загрузим, например, модель для регрессии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cefa072",
      "metadata": {
        "id": "2cefa072",
        "outputId": "2696f2a6-5c3c-47ea-a885-e7a5f4877500"
      },
      "outputs": [],
      "source": [
        "model_regression_restored = tf.keras.models.load_model('../models/RegressionModel')\n",
        "model_regression_restored.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c837c5",
      "metadata": {
        "id": "58c837c5",
        "outputId": "2f11651c-6436-4423-f520-322df13da4e1"
      },
      "outputs": [],
      "source": [
        "# используем модель\n",
        "print(mean_absolute_error(y_regression_test, model_regression_restored.predict(X_regression_test, verbose=None)))\n",
        "print(mean_squared_error(y_regression_test, model_regression_restored.predict(X_regression_test, verbose=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gW-NhwXEChTS",
      "metadata": {
        "id": "gW-NhwXEChTS"
      },
      "source": [
        "# Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-IPcUj1UCeIz",
      "metadata": {
        "id": "-IPcUj1UCeIz"
      },
      "source": [
        "<b>Традиционное предупреждение для всех лабораторных работ:</b> перед обучением моделей необходимо выполнить предварительную обработку данных, которая <b>обязательно</b> включает в себя:\n",
        "- заполнение пропущенных значений (рекомедуется логика заполнения пропусков на основе типа данных, которая использовалась в РГР по Практикуму);\n",
        "- преобразование категориальных признаков в числовые (используйте one-hot кодирование или map; используйте знания с Практикума)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jcfIwITzCgTb",
      "metadata": {
        "id": "jcfIwITzCgTb"
      },
      "source": [
        "Предобработка может включать в себя другие действия, но выполнение описанных выше действий обязательно."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H8y4nWluCkgr",
      "metadata": {
        "id": "H8y4nWluCkgr"
      },
      "source": [
        "Сделайте это один раз и сохраните в отдельный csv файл, а потом его используйте."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T24SZSN1CmBL",
      "metadata": {
        "id": "T24SZSN1CmBL"
      },
      "source": [
        "<b>Выполните следующие задания:</b>\n",
        "- решите задачи регрессии и классификации на ваших данных используя полносвязные нейронные сети; соберите их используя API Keras фреймворка TensorFlow; оцените качество полученных моделей с помощью метрик;\n",
        "- реализуйте многослойный персептрон, с помощью которого можно решать задачи регрессии и классификации; предусмотрите возможность использовать такие функции активации, как sigmoid, tanh и relu; также предусмотрите возможность указать, сколько слоев нужно, сколько на каждом из них нейронов и какую функцию активации должен иметь слой; реализуйте обучение персептрона методом обратного распространения ошибки; самостоятельно найдите производные функций sigmoid, tanh и relu; реализуйте классический градиентный спуск с возможностью указания шага.\n",
        "\n",
        "<b>Дополнительные задания:</b>\n",
        "- самостоятельно изучите отличия работы оптимизаторов Adam и RMSProp от классического градиентного спуска; реализуйте градиентный спуск с использованием указанных оптимизаторов; предусмотрите возможность использования реализованных вами оптимизаторов в вашем персептроне."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ozjLqw6KCoSK",
      "metadata": {
        "id": "ozjLqw6KCoSK"
      },
      "outputs": [],
      "source": [
        "# пишите код здесь и в ячейках ниже"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "de237088",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = pd.read_csv('c1.csv')\n",
        "X = data.drop(columns=['Fire Alarm'])\n",
        "y = data['Fire Alarm'].replace({'Yes':1, 'No':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ebd45995",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1566/1566 [==============================] - 3s 1ms/step - loss: 3066828.7500 - accuracy: 0.5931\n",
            "Epoch 2/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 1990991.6250 - accuracy: 0.5940\n",
            "Epoch 3/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 1185321.5000 - accuracy: 0.5973\n",
            "Epoch 4/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 906596.6875 - accuracy: 0.5955\n",
            "Epoch 5/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 736895.9375 - accuracy: 0.5939\n",
            "Epoch 6/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 519327.0312 - accuracy: 0.5938\n",
            "Epoch 7/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 396002.5625 - accuracy: 0.5986\n",
            "Epoch 8/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 299232.8438 - accuracy: 0.6056\n",
            "Epoch 9/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 231394.4219 - accuracy: 0.5996\n",
            "Epoch 10/10\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 153514.9375 - accuracy: 0.6021\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
            "...layers\\dense\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...layers\\dense_1\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...layers\\dense_2\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...metrics\\mean\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...metrics\\mean_metric_wrapper\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            "Keras model archive saving:\n",
            "File Name                                             Modified             Size\n",
            "config.json                                    2023-06-26 18:10:38         1846\n",
            "metadata.json                                  2023-06-26 18:10:38           64\n",
            "variables.h5                                   2023-06-26 18:10:38        85240\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Шаг 3: Создание и обучение модели\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # Определите слои модели\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e727beb7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "config.json                                    2023-06-26 18:10:38         1846\n",
            "metadata.json                                  2023-06-26 18:10:38           64\n",
            "variables.h5                                   2023-06-26 18:10:38        85240\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\\dense\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...layers\\dense_1\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...layers\\dense_2\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...metrics\\mean\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...metrics\\mean_metric_wrapper\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n"
          ]
        }
      ],
      "source": [
        "with open('model.pkl', 'rb') as file:\n",
        "    model1 = pickle.load(file)\n",
        "y_pred = [np.argmax(pred) for pred in model1.predict(X_test, verbose=None)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "31a6ba87",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e7533747",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('c1.csv')\n",
        "X = data.drop(columns=['Fire Alarm'])\n",
        "y = data['Fire Alarm'].replace({'Yes':1, 'No':0})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.2)\n",
        "\n",
        "# model_classification_1 = tf.keras.Sequential(\n",
        "#     [\n",
        "#         tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(14,)),\n",
        "#         tf.keras.layers.Dense(32, activation=\"linear\"),\n",
        "#         tf.keras.layers.Dense(16, activation=\"tanh\"),\n",
        "#         tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "#         # сначала используем 1 нейрон и sigmoid\n",
        "#         tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
        "#     ]\n",
        "# )\n",
        "# # в качестве функции активации используется бинарная  кроссэнтропия\n",
        "# model_classification_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "# # verbose=None - не будет логов\n",
        "# model_classification_1.fit(X_train, y_train, epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b162c411",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392/392 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[1;32m----> 2\u001b[0m confusion_matrix(y_test, model_classification_1\u001b[39m.\u001b[39;49mpredict(X_test))\n",
            "File \u001b[1;32md:\\ml\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
            "File \u001b[1;32md:\\ml\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     92\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     97\u001b[0m             type_true, type_pred\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
            "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "confusion_matrix(y_test, model_classification_1.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "50908a46",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392/392 [==============================] - 1s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       1.00      0.71      0.83     12526\n",
            "\n",
            "    accuracy                           0.71     12526\n",
            "   macro avg       0.50      0.36      0.42     12526\n",
            "weighted avg       1.00      0.71      0.83     12526\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\ml\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "d:\\ml\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "d:\\ml\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "predict = model_classification_1.predict(X_test)\n",
        "results = list()\n",
        "for i in predict:\n",
        "    if i[0] > i[1]:\n",
        "        results.append(0)\n",
        "    else:\n",
        "        results.append(1)\n",
        "results\n",
        "print(classification_report(results, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "f41b1052",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../../models/ClassificationModel1\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../../models/ClassificationModel1\\assets\n"
          ]
        }
      ],
      "source": [
        "model_classification_1.save('../../models/ClassificationModel1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "319d3808",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UTC</th>\n",
              "      <th>Temperature[C]</th>\n",
              "      <th>Humidity[%]</th>\n",
              "      <th>TVOC[ppb]</th>\n",
              "      <th>eCO2[ppm]</th>\n",
              "      <th>Raw H2</th>\n",
              "      <th>Raw Ethanol</th>\n",
              "      <th>Pressure[hPa]</th>\n",
              "      <th>PM1.0</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>NC0.5</th>\n",
              "      <th>NC1.0</th>\n",
              "      <th>NC2.5</th>\n",
              "      <th>CNT</th>\n",
              "      <th>Fire Alarm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1654733331</td>\n",
              "      <td>20.000</td>\n",
              "      <td>57.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>12306.0</td>\n",
              "      <td>18520</td>\n",
              "      <td>939.735</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1654733332</td>\n",
              "      <td>20.015</td>\n",
              "      <td>56.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>12345.0</td>\n",
              "      <td>18651</td>\n",
              "      <td>939.744</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1654733333</td>\n",
              "      <td>20.029</td>\n",
              "      <td>55.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>12374.0</td>\n",
              "      <td>18764</td>\n",
              "      <td>939.738</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1654733334</td>\n",
              "      <td>20.044</td>\n",
              "      <td>55.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>12390.0</td>\n",
              "      <td>18849</td>\n",
              "      <td>939.736</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1654733335</td>\n",
              "      <td>20.059</td>\n",
              "      <td>54.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>12403.0</td>\n",
              "      <td>18921</td>\n",
              "      <td>939.744</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62625</th>\n",
              "      <td>1655130047</td>\n",
              "      <td>18.438</td>\n",
              "      <td>15.79</td>\n",
              "      <td>625.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>13723.0</td>\n",
              "      <td>20569</td>\n",
              "      <td>936.670</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.65</td>\n",
              "      <td>4.32</td>\n",
              "      <td>0.673</td>\n",
              "      <td>0.015</td>\n",
              "      <td>5739</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62626</th>\n",
              "      <td>1655130048</td>\n",
              "      <td>18.653</td>\n",
              "      <td>15.87</td>\n",
              "      <td>612.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>13731.0</td>\n",
              "      <td>20588</td>\n",
              "      <td>936.678</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.63</td>\n",
              "      <td>4.18</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.015</td>\n",
              "      <td>5740</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62627</th>\n",
              "      <td>1655130049</td>\n",
              "      <td>18.867</td>\n",
              "      <td>15.84</td>\n",
              "      <td>627.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>13725.0</td>\n",
              "      <td>20582</td>\n",
              "      <td>936.687</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.60</td>\n",
              "      <td>3.95</td>\n",
              "      <td>0.617</td>\n",
              "      <td>0.014</td>\n",
              "      <td>5741</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62628</th>\n",
              "      <td>1655130050</td>\n",
              "      <td>19.083</td>\n",
              "      <td>16.04</td>\n",
              "      <td>638.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>13712.0</td>\n",
              "      <td>20566</td>\n",
              "      <td>936.680</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.59</td>\n",
              "      <td>3.92</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.014</td>\n",
              "      <td>5742</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62629</th>\n",
              "      <td>1655130051</td>\n",
              "      <td>19.299</td>\n",
              "      <td>16.52</td>\n",
              "      <td>643.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>13696.0</td>\n",
              "      <td>20543</td>\n",
              "      <td>936.676</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.59</td>\n",
              "      <td>3.90</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.014</td>\n",
              "      <td>5743</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62630 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              UTC  Temperature[C]  Humidity[%]  TVOC[ppb]  eCO2[ppm]   Raw H2  \\\n",
              "0      1654733331          20.000        57.36        0.0      400.0  12306.0   \n",
              "1      1654733332          20.015        56.67        0.0      400.0  12345.0   \n",
              "2      1654733333          20.029        55.96        0.0      400.0  12374.0   \n",
              "3      1654733334          20.044        55.28        0.0      400.0  12390.0   \n",
              "4      1654733335          20.059        54.69        0.0      400.0  12403.0   \n",
              "...           ...             ...          ...        ...        ...      ...   \n",
              "62625  1655130047          18.438        15.79      625.0      400.0  13723.0   \n",
              "62626  1655130048          18.653        15.87      612.0      400.0  13731.0   \n",
              "62627  1655130049          18.867        15.84      627.0      400.0  13725.0   \n",
              "62628  1655130050          19.083        16.04      638.0      400.0  13712.0   \n",
              "62629  1655130051          19.299        16.52      643.0      400.0  13696.0   \n",
              "\n",
              "       Raw Ethanol  Pressure[hPa]  PM1.0  PM2.5  NC0.5  NC1.0  NC2.5   CNT  \\\n",
              "0            18520        939.735   0.00   0.00   0.00  0.000  0.000     0   \n",
              "1            18651        939.744   0.00   0.00   0.00  0.000  0.000     1   \n",
              "2            18764        939.738   0.00   0.00   0.00  0.000  0.000     2   \n",
              "3            18849        939.736   0.00   0.00   0.00  0.000  0.000     3   \n",
              "4            18921        939.744   0.00   0.00   0.00  0.000  0.000     4   \n",
              "...            ...            ...    ...    ...    ...    ...    ...   ...   \n",
              "62625        20569        936.670   0.63   0.65   4.32  0.673  0.015  5739   \n",
              "62626        20588        936.678   0.61   0.63   4.18  0.652  0.015  5740   \n",
              "62627        20582        936.687   0.57   0.60   3.95  0.617  0.014  5741   \n",
              "62628        20566        936.680   0.57   0.59   3.92  0.611  0.014  5742   \n",
              "62629        20543        936.676   0.57   0.59   3.90  0.607  0.014  5743   \n",
              "\n",
              "      Fire Alarm  \n",
              "0             No  \n",
              "1             No  \n",
              "2             No  \n",
              "3             No  \n",
              "4             No  \n",
              "...          ...  \n",
              "62625         No  \n",
              "62626         No  \n",
              "62627         No  \n",
              "62628         No  \n",
              "62629         No  \n",
              "\n",
              "[62630 rows x 15 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
